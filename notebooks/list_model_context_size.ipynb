{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_available_ollama_models():\n",
    "    OLLAMA_URL=\"http://localhost:11434/api/tags\"\n",
    "    try:\n",
    "        response = requests.get(OLLAMA_URL, timeout=5)    \n",
    "        if response.status_code == 200:\n",
    "            models_data = response.json()\n",
    "            ret_val = sorted([model['name'] for model in models_data.get('models', [])])\n",
    "        else:\n",
    "            try:\n",
    "                error_message = response.json()  # Try to parse error response as JSON\n",
    "                error_detail = error_message.get('message', 'No detailed error message provided')\n",
    "            except ValueError:  # If response isn't JSON\n",
    "                error_detail = response.text  # Use plain text response if JSON parsing fails\n",
    "                \n",
    "            ret_val = {'status':'FAILED', 'status_code':response.status_code, 'message':error_detail} \n",
    "    except Exception as err:\n",
    "        ret_val = ret_val = {'status':'FAILED', 'status_code':500, 'message':str(err)}\n",
    "\n",
    "    return ret_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deepseek-r1:1.5b',\n",
       " 'deepseek-r1:32b',\n",
       " 'gemma3:27b',\n",
       " 'gemma3:latest',\n",
       " 'llama3.2:1b',\n",
       " 'llama3.3:latest',\n",
       " 'nomic-embed-text:latest',\n",
       " 'phi4:latest',\n",
       " 'qwen2.5-ctx_32k:32b',\n",
       " 'qwen2.5:32b',\n",
       " 'summary-assistant:latest']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_ollama_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deepseek-r1:1.5b', 'deepseek-r1:32b', 'gemma3:27b', 'gemma3:latest', 'llama3.2:1b', 'llama3.3:latest', 'nomic-embed-text:latest', 'phi4:latest', 'qwen2.5-ctx_32k:32b', 'qwen2.5:32b', 'summary-assistant:latest']\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "client = ollama.Client()\n",
    "models = client.list()\n",
    "model_names = sorted([model['model'] for model in models['models']])\n",
    "print(model_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-r1:1.5b          size=131,072 \tqwen2                                    \n",
      "deepseek-r1:32b           size=131,072 \tqwen2                                    \n",
      "llama3.2:1b               size=131,072 \tllama                                    \n",
      "llama3.3:latest           size=131,072 \tllama                                    \n",
      "qwen2.5-ctx_32k:32b       size=32,768 \tqwen2                                    \n",
      "qwen2.5:32b               size=32,768 \tqwen2                                    \n",
      "summary-assistant:latest  size=32,768 \tqwen2                                    \n",
      "phi4:latest               size=16,384 \tphi3                                     \n",
      "phi4:latest               size=16,384 \tphi3.rope.scaling.original               \n",
      "gemma3:27b                size=8,192 \tgemma3                                   \n",
      "gemma3:latest             size=8,192 \tgemma3                                   \n",
      "nomic-embed-text:latest   size=2,048 \tnomic-bert                               \n"
     ]
    }
   ],
   "source": [
    "model_context = []\n",
    "for model_name in model_names:\n",
    "    response = client.show(model_name)\n",
    "    for key in response.modelinfo.keys():\n",
    "        if 'context_length' in key:\n",
    "            model_context.append({'model_name':model_name, 'context_size':response.modelinfo[key], 'context':key[:-15]})\n",
    "            \n",
    "            \n",
    "#print(f\"{model_name:<25} size={response.modelinfo[key]:,} \\t{key:<40} \")\n",
    "\n",
    "sorted_models = sorted(model_context, key=lambda x: (-x['context_size'], x['model_name']))\n",
    "\n",
    "\n",
    "\n",
    "for model in sorted_models:\n",
    "    print(f\"{model['model_name']:<25} size={model['context_size']:,} \\t{model['context']:<40} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def elapsed_time(start_time, end_time = None):\n",
    "    if end_time is None:\n",
    "        end_time = time.perf_counter()\n",
    "    elapsed_time_seconds = end_time - start_time\n",
    "    minutes, seconds = divmod(elapsed_time_seconds, 60)\n",
    "    formatted_time = f\"{int(minutes)} minutes and {int(seconds)} seconds\"\n",
    "    return formatted_time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "time.sleep(62)\n",
    "formatted_time = elapsed_time(start_time)\n",
    "print(formatted_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords if not already installed\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def count_words_in_file(file_path, stop_words=None):\n",
    "    # Load stop words from NLTK if none are provided\n",
    "    if stop_words is None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Open and read the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Filter out stop words and count the remaining words\n",
    "    word_counts = {}\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "file_path = './youtube_summary/lex.txt'  # Replace with the path to your text file\n",
    "word_counts = count_words_in_file(file_path)\n",
    "\n",
    "# Print the word counts\n",
    "word_counts_list = sorted(word_counts.items(), key=lambda item: (-item[1], item[0]))\n",
    "word_counts = dict(word_counts_list)\n",
    "for word, count in word_counts.items():\n",
    "    print(f'{word}: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download NLTK resources if not already installed\n",
    "nltk.download('punkt')\n",
    "\n",
    "def find_two_word_combos(file_path):\n",
    "    # Open and read the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    \n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Generate bigrams (two-word combos)\n",
    "    bigrams = list(ngrams(words, 3))\n",
    "    \n",
    "    # Convert the bigrams to a list of string pairs (for easy readability)\n",
    "    bigram_strs = [' '.join(bigram) for bigram in bigrams]\n",
    "    \n",
    "    return bigram_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './youtube_summary/lex.txt'  # Replace with the path to your text file\n",
    "bigrams = find_two_word_combos(file_path)\n",
    "\n",
    "print(len(bigrams))\n",
    "# Print the bigrams\n",
    "for bigram in bigrams:\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deepseek-r1:1.5b', 'deepseek-r1:32b', 'gemma3:27b', 'gemma3:latest', 'llama3.2:1b', 'llama3.3:latest', 'nomic-embed-text:latest', 'phi4:latest', 'qwen2.5-ctx_32k:32b', 'qwen2.5:32b', 'summary-assistant:latest']\n",
      "True\n",
      "False\n",
      "131072\n",
      "qwen2\n"
     ]
    }
   ],
   "source": [
    "from ollama_utils import OllamaUtils\n",
    "\n",
    "ollama_utils = OllamaUtils()\n",
    "print(ollama_utils.model_names)\n",
    "print(ollama_utils.model_exists('deepseek-r1:1.5b'))\n",
    "print(ollama_utils.model_exists('deepseek-r1:1.5b-bad'))\n",
    "print(ollama_utils.model_context_size('deepseek-r1:1.5b'))\n",
    "\n",
    "print(ollama_utils.model_base_model('deepseek-r1:1.5b'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
